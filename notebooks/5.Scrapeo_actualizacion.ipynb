{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alexc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at facebook/dino-vitb16 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "# Añadimos la carpeta que contiene nuestro .py al path de Python\n",
    "sys.path.append(\"../src/\")\n",
    "import support_scrap as ss\n",
    "import support_mongo as sm\n",
    "import support_filter_models as sfm\n",
    "import support_tag as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar datos bbdd\n",
    "bd=sm.conectar_a_mongo(\"PullnBearData\")\n",
    "nombre_coleccion1=\"modelos_pull_hombre_tageados\"\n",
    "nombre_coleccion2=\"productos_pull_hombre_tageados\"\n",
    "df_modelos_base = sm.importar_a_dataframe(bd, nombre_coleccion1)\n",
    "df_base = sm.importar_a_dataframe(bd, nombre_coleccion2)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "print(df_base.shape)\n",
    "df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs a procesar\n",
    "listaurl = [\n",
    "    \"https://www.pullandbear.com/es/hombre/ropa/camisetas-n6323\",\n",
    "    \"https://www.pullandbear.com/es/hombre/rebajas/ropa/camisetas-y-polos-n7087\",\n",
    "    \"https://www.pullandbear.com/es/hombre/ropa/camisas-n6313\",\n",
    "    \"https://www.pullandbear.com/es/hombre/rebajas/ropa/camisas-n7088\",\n",
    "    \"https://www.pullandbear.com/es/hombre/ropa/punto-n6372\",\n",
    "    \"https://www.pullandbear.com/es/hombre/rebajas/ropa/punto-n7090\",\n",
    "    \"https://www.pullandbear.com/es/hombre/ropa/sudaderas-n6382\",\n",
    "    \"https://www.pullandbear.com/es/hombre/rebajas/ropa/sudaderas-n7089\",\n",
    "    \"https://www.pullandbear.com/es/hombre/ropa/pantalones-n6363\",\n",
    "    \"https://www.pullandbear.com/es/hombre/rebajas/ropa/pantalones-n7091\",\n",
    "    \"https://www.pullandbear.com/es/hombre/ropa/jeans-n6347\",\n",
    "    \"https://www.pullandbear.com/es/hombre/rebajas/ropa/jeans-n7818\"\n",
    "]\n",
    "\n",
    "df_nuevo = ss.process_and_extract_data_main(listaurl)\n",
    "\n",
    "# Mostrar articulos antes y despues\n",
    "print(\"Articulos base:\")\n",
    "print(df_base.shape)\n",
    "print(\"Articulos actuales:\")\n",
    "print(df_nuevo.shape)\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "df_nuevo.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPROBACIÓN DE ESTADO CON LA BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir la columna de stock al df_base\n",
    "# Si la URL está en df_nuevo, significa que está en stock; de lo contrario, no lo está\n",
    "df_base['Stock Status'] = df_base['url'].apply(\n",
    "    lambda x: 'In Stock' if x in df_nuevo['url'].values else 'Out of Stock')\n",
    "\n",
    "# Actualizar los precios en df_base si el precio en df_nuevo es diferente\n",
    "for index, row in df_base.iterrows():\n",
    "    if row['url'] in df_nuevo['url'].values:\n",
    "        # Tomar el precio de df_nuevo (current_price)\n",
    "        nuevo_precio = df_nuevo.loc[df_nuevo['url'] == row['url'], 'product_price'].values[0]\n",
    "        \n",
    "        # Comparar con el precio actual en df_base (product_price)\n",
    "        if row['product_price'] != nuevo_precio:\n",
    "            df_base.at[index, 'product_price'] = nuevo_precio\n",
    "\n",
    "# Nuevas URLs en df_nuevo que no están en df_base\n",
    "new_urls = df_nuevo[~df_nuevo['url'].isin(df_base['url'])]\n",
    "\n",
    "# URLs en df_base que no están en df_nuevo (para marcar como \"out of stock\")\n",
    "out_of_stock_urls = df_base[~df_base['url'].isin(df_nuevo['url'])]\n",
    "\n",
    "# Guardar los resultados en CSV\n",
    "new_urls.to_csv('../results/new_urls.csv', index=False)\n",
    "out_of_stock_urls.to_csv('../results/out_of_stock_urls.csv', index=False)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Nuevas URLs en df_nuevo que no están en df_base:\")\n",
    "print(len(new_urls))\n",
    "print(\"\\nURLs en df_base que no están en df_nuevo (out of stock):\")\n",
    "print(len(out_of_stock_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapeo de las nuevas urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutamos la funcion de scrapeo de productos\n",
    "df_results=ss.process_urls_products(new_urls)\n",
    "\n",
    "# Unir los DataFrames utilizando la columna 'url'\n",
    "df_results=pd.merge(df_results,new_urls,on='url', how='inner')\n",
    "df_successful = pd.concat([df_base, df_results], axis=0, ignore_index=True)\n",
    "\n",
    "# Columnas clave para identificar duplicados\n",
    "columnas_clave = ['product_price','description','current_price','color','image_url','mpn','reference_code','category_id']\n",
    "\n",
    "# Crear un DataFrame con los duplicados eliminados\n",
    "df_sin_duplicados = df_successful.drop_duplicates(subset=columnas_clave, keep='first')\n",
    "\n",
    "# Identificar las filas descartadas como la diferencia entre los originales y los sin duplicados\n",
    "descartes = df_successful.loc[~df_successful.index.isin(df_sin_duplicados.index)]\n",
    "\n",
    "# Guardar los descartes en un archivo CSV\n",
    "descartes.to_csv('../results/articulos_descartados.csv', index=False)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Articulos de tras actualización:\")\n",
    "print(df_successful.shape)\n",
    "print(f\"Se guardaron {len(descartes)} filas descartadas en 'descartes.csv'.\")\n",
    "print(\"Articulos sin duplicados:\")\n",
    "print(df_sin_duplicados.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMOGENEIZAR LOS DATOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una nueva columna con el color homogeneizado\n",
    "df_sin_duplicados['color_homogeneizado'] = df_sin_duplicados['color'].apply(ss.homogeneizar_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CATEGORÍA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una nueva columna con la categoría de la prenda\n",
    "df_sin_duplicados['Categoria'] = df_sin_duplicados['product_name'].apply(ss.categorizar_ropa)\n",
    "\n",
    "# Mostrar las primeras filas para comprobar\n",
    "df_sin_duplicados[['product_name', 'Categoria']].sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTROLAR URLs FALTANTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignar \"out of stock\" a la columna 'Stock Status' donde 'image_url' sea vacío o nulo\n",
    "df_sin_duplicados.loc[(df_sin_duplicados['image_url'].isna()) | (df_sin_duplicados['image_url'] == ''), 'Stock Status'] = 'Out of Stock'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREAR DF DE MODELOS NUEVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar los pantalones para coger los modelos de cuerpo entero\n",
    "df_modelos_nuevo = df_sin_duplicados[(df_sin_duplicados['Categoria'] == 'Pantalón') & (df_sin_duplicados['Stock Status'] == 'In Stock')] \n",
    "df_modelos_nuevo[\"Fondo\"] =df_modelos_nuevo[\"image1_url\"].apply(sfm.is_white_background)\n",
    "df_modelos_nuevo[\"Modelo\"] =df_modelos_nuevo[\"image1_url\"].apply(sfm.classify_image)\n",
    "df_modelos_final=df_modelos_nuevo[(df_modelos_nuevo[\"Fondo\"] == \"Blanco\")&(df_modelos_nuevo[\"Modelo\"] == \"Modelo\")&(df_modelos_nuevo[\"Stock Status\"] == \"In stock\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GUARDAR CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el nuevo DataFrame con pantalones en un archivo CSV\n",
    "df_modelos_final.to_csv('../results/Modelos_enteros.csv', index=False)\n",
    "# Guardar los cambios en un archivo nuevo si lo necesitas\n",
    "df_sin_duplicados.to_csv('../results/all_products_info_with_categories.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos los datos en la bbdd de Mongo\n",
    "bd=sm.conectar_a_mongo(\"PullnBearData\")\n",
    "nombre_coleccion1=\"modelos_pull_hombre_tageados\"\n",
    "nombre_coleccion2=\"productos_pull_hombre_tageados\"\n",
    "sm.eliminar_coleccion(bd, nombre_coleccion1)\n",
    "sm.subir_dataframe_a_mongo(bd, df_modelos_final, nombre_coleccion1)\n",
    "sm.eliminar_coleccion(bd, nombre_coleccion2)\n",
    "sm.subir_dataframe_a_mongo(bd, df_sin_duplicados, nombre_coleccion2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
